{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Binarized NN with MaxSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from pysat.formula import WCNF\n",
    "from pysat.examples.fm import FM\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarized NN without hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation with different binary fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # Input neurons\n",
    "M = 5 # Output neurons\n",
    "p_test = 0.3 # Percentage of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all possible combinations for dataset in case of the input size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = list(itertools.product([-1, 1], repeat=N))\n",
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical fucntions of different complexity for the input size N=5 and the output size M=5. Consider to adjust them for the different dimentionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_function_1(X):\n",
    "    return [\n",
    "        (\n",
    "            x[3],  \n",
    "            x[1] or x[2],  # OR operation\n",
    "            x[0], \n",
    "            -x[3],  # NOT operation\n",
    "            x[3] and x[4]  # AND operation\n",
    "        )\n",
    "        for x in X\n",
    "    ]\n",
    "\n",
    "\n",
    "def logical_function_2(X):\n",
    "    return [\n",
    "        (\n",
    "            x[0] and x[1],  # AND operation\n",
    "            x[2] or x[3],  # OR operation\n",
    "            (x[2] and -x[3]) or (-x[2] and x[3]),  # XOR \n",
    "            -x[3],  # NOT operation\n",
    "            (x[4] and x[0]) or x[1]  # Complex OR-AND combination\n",
    "        )\n",
    "        for x in X\n",
    "    ]\n",
    "\n",
    "\n",
    "def logical_function_3(X):\n",
    "    return [\n",
    "        (\n",
    "            (x[0] and x[1]) or ((x[2] and -x[3]) or (-x[2] and x[3])),  # Combination of AND, OR, XOR\n",
    "            (x[1] or x[2]) and (-x[3] or x[4]),  # OR and NOT combination\n",
    "            x[0] or (x[1] and x[2] and -x[3]) or (-x[1] and -x[2] and x[3]),  # XOR mixed with AND and OR\n",
    "            (x[3] and x[4]) or (-x[0] and -x[1]) or (x[0] and x[1]),  # Mix of XOR, NOT, and OR\n",
    "            ((x[2] and x[3]) or (x[4] and -x[0]) or (-x[4] and x[0])) and x[1]  # More nested operations\n",
    "        )\n",
    "        for x in X\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate outputs for different complexity levels\n",
    "Y1 = logical_function_1(X)\n",
    "Y2 = logical_function_2(X)\n",
    "Y3 = logical_function_3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train1, Y_test1 = train_test_split(X, Y1, test_size=p_test, random_state=42)\n",
    "X_train, X_test, Y_train2, Y_test2 = train_test_split(X, Y2, test_size=p_test, random_state=42)\n",
    "X_train, X_test, Y_train3, Y_test3 = train_test_split(X, Y3, test_size=p_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train size\n",
    "train_size = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAT Encoding by blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute SAT encoding for various indices\n",
    "def SAT_encoding(i, n, N, offset = 0):\n",
    "   \"\"\"\n",
    "   Computes the SAT encoding index for inputs, weights, or other components.\n",
    "\n",
    "   Parameters:\n",
    "   i: int, Represents the current index (e.g., sample index in a training set).\n",
    "   n int, Represents the specific neuron or weight index within the layer.\n",
    "   N : int, The total number of elements in a set (e.g., number of input neurons).\n",
    "   offset : int, An offset to shift index values appropriately for different sections (default is 0).\n",
    "\n",
    "   Returns:\n",
    "   int, The computed SAT encoding index.\n",
    "   \"\"\"\n",
    "   return offset + i*N + n + 1\n",
    "\n",
    "# Function to encode input neurons\n",
    "def x_encod_i_n(i, n):\n",
    "    \"\"\"\n",
    "    Encodes the SAT index for an input neuron.\n",
    "\n",
    "    Parameters:\n",
    "    i : int, Index of the training sample.\n",
    "    n : int, Index of the input neuron.\n",
    "\n",
    "    Returns:\n",
    "    int, The computed SAT index for input neurons.\n",
    "    \"\"\"\n",
    "    return SAT_encoding(i, n, N, offset = 0)\n",
    "\n",
    "# Function to encode weights connecting neurons\n",
    "def w_encode_n_p(n, m):\n",
    "    \"\"\"\n",
    "    Encodes the SAT index for weights between neurons.\n",
    "\n",
    "    Parameters:\n",
    "    n : int, Index of the neuron in the previous layer.\n",
    "    m : int, Index of the neuron in the next layer.\n",
    "\n",
    "    Returns:\n",
    "    int, The computed SAT index for the weight connecting neurons.\n",
    "    \"\"\"\n",
    "    return SAT_encoding(n, m, M, offset = train_size * N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted CNF for training\n",
    "\n",
    "We add both hard (input neurons) and soft (the punishement for misclassification) constarints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcnf = WCNF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1], [2], [-3], [-4], [-5], [-6], [-7], [-8], [-9], [10], [-11], [12], [-13], [-14], [15], [-16], [17], [18], [-19], [20], [21], [-22], [-23], [-24], [-25], [26], [-27], [-28], [29], [-30], [-31], [32], [33], [34], [35], [36], [37], [-38], [-39], [-40], [-41], [-42], [43], [44], [45], [46], [-47], [-48], [-49], [50], [51], [52], [-53], [54], [55], [-56], [57], [-58], [59], [-60], [61], [-62], [63], [64], [-65], [66], [67], [-68], [69], [-70], [-71], [-72], [73], [74], [-75], [76], [-77], [78], [-79], [80], [81], [82], [-83], [-84], [85], [-86], [-87], [88], [-89], [-90], [-91], [92], [-93], [94], [95], [-96], [97], [98], [99], [-100], [-101], [-102], [-103], [104], [105], [106], [107], [108], [109], [-110]]\n"
     ]
    }
   ],
   "source": [
    "# Hard constarints\n",
    "for i in range(train_size):\n",
    "    for j in range(N):\n",
    "        if X_train[i][j] > 0:\n",
    "            wcnf.append([x_encod_i_n(i,j)])\n",
    "        else:\n",
    "            wcnf.append([-x_encod_i_n(i,j)])\n",
    "\n",
    "print(wcnf.hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than half inputs for soft constarints\n",
    "N_half = N//2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comb = list(itertools.combinations(range(N), N_half)) # More than half of inputs\n",
    "neg_comb = list(itertools.product([-1, 1],  repeat = N_half)) # With negation or not\n",
    "\n",
    "soft_combs = list(itertools.product(input_comb, neg_comb)) # Combinations for soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft constraints\n",
    "def add_soft_const(wcnf, y_train):\n",
    "    \"\"\"\n",
    "    Adds soft constraints to the weighted CNF formulation.\n",
    "    \n",
    "    Parameters:\n",
    "    - wcnf: WCNF object with hard constraints already added.\n",
    "    - y_train: Training labels for the dataset.\n",
    "\n",
    "    The function enforces that the weight assignments align with the expected output,\n",
    "    encouraging correct classification while allowing some flexibility (soft constraints).\n",
    "    \"\"\"\n",
    "    for i in range(train_size):   # Iterate over each training sample\n",
    "        for m in range(M):   # Iterate over each output neuron\n",
    "\n",
    "            # If the expected output is 1, we enforce positive influence\n",
    "            if y_train[i][m] == 1:\n",
    "                for inputs, signs in soft_combs:   # Iterate over all possible input combinations\n",
    "                    clause = []\n",
    "                    for n_num, n in enumerate(inputs):   # Iterate over selected input neurons\n",
    "                        clause.append(signs[n_num]*x_encod_i_n(i,n))   # Encode input neurons\n",
    "                        clause.append(-signs[n_num]*w_encode_n_p(n,m))   # Ensure weights align with input\n",
    "                    wcnf.append(clause, weight=1)\n",
    "\n",
    "            # If the expected output is -1, we enforce negative influence\n",
    "            if y_train[i][m] == -1:\n",
    "                for inputs, signs in soft_combs:   # Iterate over all possible input combinations\n",
    "                    clause = []\n",
    "                    for n_num, n in enumerate(inputs):  # Iterate over selected input neurons\n",
    "                        clause.append(signs[n_num]*x_encod_i_n(i,n))   # Encode input neurons\n",
    "                        clause.append(signs[n_num]*w_encode_n_p(n,m))    # Ensure weights align with input\n",
    "                    wcnf.append(clause, weight=1)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of soft constraints:  8800\n"
     ]
    }
   ],
   "source": [
    "wcnf_cp = wcnf.copy()\n",
    "add_soft_const(wcnf_cp, Y_train1)\n",
    "print(\"Number of soft constraints: \", len(wcnf_cp.soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver cost:  27\n"
     ]
    }
   ],
   "source": [
    "solver = FM(wcnf_cp)\n",
    "solver.compute()\n",
    "model = solver.model\n",
    "print(\"Solver cost: \", solver.cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "W = np.sign(model[train_size*N :]).reshape(N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1,  1, -1],\n",
       "       [ 1,  1, -1, -1, -1],\n",
       "       [ 1, -1, -1, -1, -1],\n",
       "       [ 1,  1,  1, -1, -1],\n",
       "       [-1, -1, -1,  1,  1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "pred_train = np.sign(np.matmul(X_train, W))\n",
    "pred_test = np.sign(np.matmul(X_test, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7545454545454545\n",
      "Test Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {accuracy_score(list(itertools.chain(*Y_train1)), list(itertools.chain(*pred_train)))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(list(itertools.chain(*Y_test1)), list(itertools.chain(*pred_test)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 1:\n",
      "Solver cost:  27\n",
      "Training Accuracy: 0.7545454545454545\n",
      "Test Accuracy: 0.54\n",
      "\n",
      "Function 2:\n",
      "Solver cost:  27\n",
      "Training Accuracy: 0.7545454545454545\n",
      "Test Accuracy: 0.54\n",
      "\n",
      "Function 3:\n",
      "Solver cost:  26\n",
      "Training Accuracy: 0.7636363636363637\n",
      "Test Accuracy: 0.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test_list = [Y_test1, Y_test2, Y_test3]\n",
    "for i, y_train in enumerate([Y_train1, Y_train2, Y_train3]):\n",
    "    print(f\"Function {i + 1}:\")\n",
    "\n",
    "    wcnf_cp = wcnf.copy()\n",
    "    add_soft_const(wcnf_cp, y_train)\n",
    "\n",
    "    solver = FM(wcnf_cp)\n",
    "    solver.compute()\n",
    "    model = solver.model\n",
    "    print(\"Solver cost: \", solver.cost) \n",
    "\n",
    "    W = np.sign(model[train_size*N :]).reshape(N, M)\n",
    "\n",
    "    pred_train = np.sign(np.matmul(X_train, W))\n",
    "    pred_test = np.sign(np.matmul(X_test, W))\n",
    "\n",
    "    print(f\"Training Accuracy: {accuracy_score(list(itertools.chain(*y_train)), list(itertools.chain(*pred_train)))}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(list(itertools.chain(*Y_test_list[i])), list(itertools.chain(*pred_test)))}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarized NN with hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation with different binary fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # Input neurons\n",
    "H = 3 # Hidden neurons\n",
    "M = 3 # Output neurons -> have to reduce the number, otherwise not enough RAM to train (15 Gb available)\n",
    "p_test = 0.3 # Percentage of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all possible combinations for dataset in case of the input size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = list(itertools.product([-1, 1], repeat=N))\n",
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical fucntions of different complexity for the input size N=5 and the output size M=5. Consider to adjust them for the different dimentionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_function_1(X):\n",
    "    return [\n",
    "        (\n",
    "            x[1] or x[2],  # OR operation\n",
    "            -x[0],  # NOT operation\n",
    "            x[3] and x[4]  # AND operation\n",
    "        )\n",
    "        for x in X\n",
    "    ]\n",
    "\n",
    "\n",
    "def logical_function_2(X):\n",
    "    return [\n",
    "        (\n",
    "            x[4] and x[1],  # AND operation\n",
    "            (x[2] and -x[3]) or (-x[2] and x[3]),  # XOR \n",
    "            (x[4] and x[0]) or x[1]  # Complex OR-AND-NOT combination\n",
    "        )\n",
    "        for x in X\n",
    "    ]\n",
    "\n",
    "\n",
    "def logical_function_3(X):\n",
    "    return [\n",
    "        (\n",
    "            (x[0] and x[1]) or ((x[2] and -x[3]) or (-x[2] and x[3])),  # Combination of AND, OR, XOR\n",
    "            x[0] or (x[1] and x[2] and -x[3]) or (-x[1] and -x[2] and x[3]),  # XOR mixed with AND and OR\n",
    "            ((x[2] and x[3]) or (x[4] and -x[0]) or (-x[4] and x[0])) and x[1]  # More nested operations\n",
    "        )\n",
    "        for x in X\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate outputs for different complexity levels\n",
    "Y1 = logical_function_1(X)\n",
    "Y2 = logical_function_2(X)\n",
    "Y3 = logical_function_3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train1, Y_test1 = train_test_split(X, Y1, test_size=p_test, random_state=42)\n",
    "X_train, X_test, Y_train2, Y_test2 = train_test_split(X, Y2, test_size=p_test, random_state=42)\n",
    "X_train, X_test, Y_train3, Y_test3 = train_test_split(X, Y3, test_size=p_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train size\n",
    "train_size = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAT Encoding by blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to encode weights connecting neurons\n",
    "\n",
    "# Input -> Hidden\n",
    "def w1_encode_n_h(n, h):\n",
    "    \"\"\"\n",
    "    Encodes the SAT index for weights between neurons.\n",
    "\n",
    "    Parameters:\n",
    "    n : int, Index of the neuron in the previous layer.\n",
    "    h : int, Index of the neuron in the next layer.\n",
    "\n",
    "    Returns:\n",
    "    int, The computed SAT index for the weight connecting neurons.\n",
    "    \"\"\"\n",
    "    return SAT_encoding(n, h, H, offset = train_size * N)\n",
    "\n",
    "# Hidden -> Output\n",
    "def w2_encode_h_m(h, m):\n",
    "    \"\"\"\n",
    "    Encodes the SAT index for weights between neurons.\n",
    "\n",
    "    Parameters:\n",
    "    h : int, Index of the neuron in the previous layer.\n",
    "    m : int, Index of the neuron in the next layer.\n",
    "\n",
    "    Returns:\n",
    "    int, The computed SAT index for the weight connecting neurons.\n",
    "    \"\"\"\n",
    "    return SAT_encoding(h, m, M, offset = train_size * N + N * H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted CNF for training\n",
    "\n",
    "We add both hard (input neurons) and soft (the punishement for misclassification) constarints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcnf = WCNF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1], [2], [-3], [-4], [-5], [-6], [-7], [-8], [-9], [10], [-11], [12], [-13], [-14], [15], [-16], [17], [18], [-19], [20], [21], [-22], [-23], [-24], [-25], [26], [-27], [-28], [29], [-30], [-31], [32], [33], [34], [35], [36], [37], [-38], [-39], [-40], [-41], [-42], [43], [44], [45], [46], [-47], [-48], [-49], [50], [51], [52], [-53], [54], [55], [-56], [57], [-58], [59], [-60], [61], [-62], [63], [64], [-65], [66], [67], [-68], [69], [-70], [-71], [-72], [73], [74], [-75], [76], [-77], [78], [-79], [80], [81], [82], [-83], [-84], [85], [-86], [-87], [88], [-89], [-90], [-91], [92], [-93], [94], [95], [-96], [97], [98], [99], [-100], [-101], [-102], [-103], [104], [105], [106], [107], [108], [109], [-110]]\n"
     ]
    }
   ],
   "source": [
    "# Hard constarints\n",
    "for i in range(train_size):\n",
    "    for j in range(N):\n",
    "        if X_train[i][j] > 0:\n",
    "            wcnf.append([x_encod_i_n(i,j)])\n",
    "        else:\n",
    "            wcnf.append([-x_encod_i_n(i,j)])\n",
    "            \n",
    "print(wcnf.hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than half inputs for soft constarints\n",
    "N_half = N//2+1\n",
    "H_half = H//2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comb = list(itertools.combinations(range(N), N_half)) # More than half of inputs\n",
    "neg_comb_inp = list(itertools.product([-1, 1],  repeat = N_half)) # With negation or not\n",
    "soft_combs_inp = list(itertools.product(input_comb, neg_comb_inp)) # Combinations for soft constraints\n",
    "\n",
    "hidden_comb = list(itertools.combinations(range(H), H_half)) # More than half of hidden neurons\n",
    "neg_comb_hid = list(itertools.product([-1, 1],  repeat = H_half)) # With negation or not\n",
    "soft_combs_hid = list(itertools.product(hidden_comb, neg_comb_hid)) # Combinations for soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft constraints\n",
    "def add_soft_const(wcnf, y_train):\n",
    "    \"\"\"\n",
    "    Adds soft constraints to the Weighted CNF formulation for a Binarized Neural Network with a hidden layer.\n",
    "\n",
    "    Parameters:\n",
    "    - wcnf: WCNF object with hard constraints already added.\n",
    "    - y_train: Training labels for the dataset.\n",
    "\n",
    "    The function enforces that the weight assignments allow for correct activations \n",
    "    at both the hidden and output layers while minimizing constraint violations.\n",
    "    \"\"\"\n",
    "    for i in range(train_size):   # Iterate over each training sample\n",
    "        for m in range(M):   # Iterate over each output neuron\n",
    "\n",
    "             # Checks correctness of outputs\n",
    "            if y_train[i][m] not in [1, -1]:\n",
    "                continue  \n",
    "            \n",
    "            sign_m = y_train[i][m]    # Store the sign for easier reference\n",
    "\n",
    "            for inputs_h, signs_h in soft_combs_hid:   # Iterate over hidden neuron activation patterns\n",
    "                w2_clause_parts = []    # Store parts of the final clause\n",
    "                \n",
    "                for h_num, h in enumerate(inputs_h):   # Iterate over selected hidden neurons\n",
    "                    h_clauses = []   # Store clauses for hidden neuron activation\n",
    "                    \n",
    "                    for inputs_n, signs_n in soft_combs_inp:    # Iterate over input combinations affecting hidden neurons\n",
    "                        h_clause = []\n",
    "\n",
    "                        for n_num, n in enumerate(inputs_n):    # Iterate over selected input neurons\n",
    "                            h_clause.append(signs_n[n_num]*x_encod_i_n(i,n))    # Encode input neuron\n",
    "\n",
    "                            # Handle sign logic for weight between input and hidden layer\n",
    "                            if signs_h[h_num] == 1:\n",
    "                                h_clause.append(-signs_n[n_num] * w1_encode_n_h(n, h))   # Negative for correct alignment\n",
    "                            else:\n",
    "                                h_clause.append(signs_n[n_num] * w1_encode_n_h(n, h))   # Positive for incorrect alignment\n",
    "\n",
    "                        h_clauses.append(h_clause)\n",
    "\n",
    "                    # Append W2 encoding (hidden to output) with the correct sign\n",
    "                    h_clauses_plus_w2 = [sublist + [-sign_m * signs_h[h_num] * w2_encode_h_m(h, m)] for sublist in h_clauses]\n",
    "                    w2_clause_parts.append(h_clauses_plus_w2)\n",
    "                \n",
    "                # Generate the final set of clauses by combining hidden layer constraints\n",
    "                clauses = [sum(combo, []) for combo in itertools.product(*w2_clause_parts)]\n",
    "                \n",
    "                # Append each generated clause to the weighted CNF\n",
    "                for clause in clauses:\n",
    "                    wcnf.append(clause, weight=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of soft constraints:  5068800\n"
     ]
    }
   ],
   "source": [
    "wcnf_cp = wcnf.copy()\n",
    "add_soft_const(wcnf_cp, Y_train1)\n",
    "print(\"Number of soft constraints: \", len(wcnf_cp.soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -111, -2, -114, -3, -117, 126, -1, -112, -2, -115, -3, -118, 129]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcnf_cp.soft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "solver = FM(wcnf_cp)\n",
    "solver.compute()\n",
    "model = solver.model\n",
    "print(solver.cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "W1 = np.sign(model[train_size*N : train_size*N + N*H]).reshape(N, H)\n",
    "W2 = np.sign(model[train_size*N + N*H :]).reshape(H, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = np.sign(np.matmul(np.matmul(X_train, W1), W2))\n",
    "pred_test = np.sign(np.matmul(np.matmul(X_test, W1), W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {accuracy_score(list(itertools.chain(*Y_train1)), list(itertools.chain(*pred_train)))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(list(itertools.chain(*Y_test1)), list(itertools.chain(*pred_test)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 1:\n",
      "Solver cost:  2\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.8\n",
      "\n",
      "Function 2:\n",
      "Solver cost:  4\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.8\n",
      "\n",
      "Function 3:\n",
      "Solver cost:  1\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test_list = [Y_test1, Y_test2, Y_test3]\n",
    "for i, y_train in enumerate([Y_train1, Y_train2, Y_train3]):\n",
    "    print(f\"Function {i + 1}:\")\n",
    "\n",
    "    wcnf_cp = wcnf.copy()\n",
    "    add_soft_const(wcnf_cp, y_train)\n",
    "\n",
    "    solver = FM(wcnf_cp)\n",
    "    solver.compute()\n",
    "    model = solver.model\n",
    "    print(\"Solver cost: \", solver.cost) \n",
    "\n",
    "    W1 = np.sign(model[train_size*N : train_size*N + N*H]).reshape(N, H)\n",
    "    W2 = np.sign(model[train_size*N + N*H :]).reshape(H, M)\n",
    "\n",
    "\n",
    "    pred_train = np.sign(np.matmul(np.matmul(X_train, W1), W2))\n",
    "    pred_test = np.sign(np.matmul(np.matmul(X_test, W1), W2))\n",
    "\n",
    "    print(f\"Training Accuracy: {accuracy_score(list(itertools.chain(*y_train)), list(itertools.chain(*pred_train)))}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(list(itertools.chain(*Y_test_list[i])), list(itertools.chain(*pred_test)))}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
